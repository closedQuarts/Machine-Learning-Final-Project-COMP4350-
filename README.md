Customer Lifetime Value (CLV) Prediction

This project builds a machine learning pipeline to predict Customer Lifetime Value (CLV) using real-world e-commerce transaction data. It includes data preprocessing, feature engineering, multiple model evaluations, hyperparameter optimization, visualization, and computational complexity analysis.

â¸»

ğŸ“ Project Structure

â”œâ”€â”€ data/                # Input dataset (CSV uploaded via Colab)
â”œâ”€â”€ notebook/            # Colab notebook (.ipynb)
â”œâ”€â”€ README.md            # Project documentation


â¸»

ğŸ“Œ Objectives
	â€¢	Predict Customer Lifetime Value (CLV)
	â€¢	Engineer meaningful customer-level behavioral features
	â€¢	Compare baseline and advanced ML models
	â€¢	Optimize model performance using GridSearchCV
	â€¢	Visualize predictions
	â€¢	Analyze time and memory complexity

â¸»

ğŸ“¥ 1. Data Loading

The dataset is uploaded through Google Colab using:

from google.colab import files
uploaded = files.upload()
data = pd.read_csv(file_name, encoding='latin1')

Initial preprocessing includes:
	â€¢	Converting InvoiceDate â†’ datetime
	â€¢	Creating TotalSpend = Quantity Ã— UnitPrice
	â€¢	Handling missing values

â¸»

ğŸ› ï¸ 2. Feature Engineering

Customer-level aggregated features are generated:
	â€¢	Recency â€“ days since last purchase
	â€¢	TransactionFrequency â€“ number of unique invoices
	â€¢	TotalSpend â€“ total revenue
	â€¢	UniqueProductsPurchased â€“ product variety
	â€¢	AvgSpendPerTransaction â€“ average purchase value

These features form the core inputs for the ML models.

â¸»

ğŸ¤– 3. Modeling

Models trained:
	â€¢	Random Forest Regressor
	â€¢	Linear Regression (baseline)

Before training:

X_train, X_test, y_train, y_test = train_test_split(...)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

Performance Metrics
	â€¢	MAE (Mean Absolute Error)
	â€¢	MSE (Mean Squared Error)
	â€¢	RÂ² Score

Random Forest outperformed Linear Regression, showing stronger predictive ability.

â¸»

âš™ï¸ 4. Hyperparameter Optimization

GridSearchCV tested multiple configurations of Random Forest:

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

Best Parameters:

{
  'n_estimators': 200,
  'max_depth': 10,
  'min_samples_split': 2,
  'min_samples_leaf': 4
}


â¸»

ğŸ“Š 5. Visualization

A scatter plot compares actual vs predicted CLV:

plt.scatter(y_test, y_pred_best)
plt.plot(y_test, y_test, linestyle='--')

This visualization highlights prediction accuracyâ€”points closer to the diagonal represent better performance.

â¸»

â±ï¸ 6. Time & Space Complexity

Performance comparison:

Model	Training Time	Prediction Time	Memory Usage	Notes
Linear Regression	Very fast	Extremely fast	Low	Poor accuracy
Random Forest	Slower	Moderate	Higher	Best accuracy

Random Forest requires more computation but delivers substantially better results.

â¸»

ğŸ“Œ Conclusion

This project demonstrates a complete Machine Learning workflow for CLV prediction.
Key outcomes:
	â€¢	Feature engineering significantly boosts predictive power
	â€¢	Random Forest is the most suitable model
	â€¢	Hyperparameter tuning improves accuracy further
	â€¢	Visual analysis supports model reliability

This pipeline can be adapted for real-world CLV prediction in e-commerce or CRM systems.

â¸»

ğŸš€ Future Improvements
	â€¢	Add XGBoost & LightGBM comparisons
	â€¢	Deploy the model via Flask/FastAPI
	â€¢	Automate feature engineering
	â€¢	Add cross-validation learning curves

â¸»

ğŸ“„ License

This project is free to use for educational and research purposes.

â¸»
